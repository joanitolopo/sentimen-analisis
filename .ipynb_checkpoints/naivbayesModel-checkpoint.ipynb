{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset yang digunakan dapat didownload di: https://github.com/rizalespe/Dataset-Sentimen-Analisis-Bahasa-Indonesia atau menggunakan ***git clone*** seperti contoh dibawah ini. Folder yang di _clone_ tersimpan ke dalam folder tempat file project ini disimpan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uQhkN4DG2gXb",
    "outputId": "120f7340-a07e-4a82-b0a2-7b7de9bc2337"
   },
   "outputs": [],
   "source": [
    "#!git clone https://github.com/rizalespe/Dataset-Sentimen-Analisis-Bahasa-Indonesia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQLN-l93iF2B"
   },
   "source": [
    "## Install Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Requirement Package**:\n",
    "\n",
    "```\n",
    "1. nltk : https://www.nltk.org/\n",
    "2. Sastrawi: https://github.com/sastrawi/sastrawi\n",
    "3. numpy: https://numpy.org/\n",
    "4. pandas: https://pandas.pydata.org/\n",
    "5. sklearn: https://scikit-learn.org/stable/\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZMhPthAicoE"
   },
   "source": [
    "# Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install Sastrawi\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uq4zAXNPih6-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "from string import punctuation\n",
    "import os\n",
    "import json\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nh5funiyhB5G"
   },
   "source": [
    "# ```{Utils}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "UW6_M8jhgyy-"
   },
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    \n",
    "    # kumpulan stemming\n",
    "    factory_stem = StemmerFactory()\n",
    "    stemmer = factory_stem.create_stemmer()\n",
    "\n",
    "    # kumpulan stopwords\n",
    "    factory_stopwords = StopWordRemoverFactory()\n",
    "    stopword = factory_stopwords.get_stop_words() + stopwords.words('indonesian')\n",
    "  \n",
    "    # menghapus kata-kata yang tidak penting seperti @, #\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    tweet = re.sub(r'<USERNAME>', '', tweet)\n",
    "    \n",
    "    # tokenizer word\n",
    "    tweet_tokens = word_tokenize(tweet)\n",
    "    \n",
    "    # membersihkan word\n",
    "    tweets_clean = [stemmer.stem(word) for word in tweet_tokens if (word not in stopword and word not in punctuation)]\n",
    "  \n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup(freqs, word, label):\n",
    "    \n",
    "    n = 0\n",
    "    \n",
    "    pair = (word, label)\n",
    "    if (pair in freqs):\n",
    "        n = freqs[pair]\n",
    "        \n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cKzrfhkFqV3K"
   },
   "outputs": [],
   "source": [
    "def count_tweets(tweets, ys):\n",
    "    \n",
    "    yslist = np.squeeze(ys).tolist()\n",
    "    \n",
    "    freqs = {}\n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, y)\n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            else:\n",
    "                freqs[pair] = 1\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hmi7dW_Ttakm"
   },
   "source": [
    "# Processing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLVQNDWptjJF"
   },
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OnmXDdv7ia_u"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/dataset_komentar_instagram_cyberbullying.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "upLfzDUrihHG",
    "outputId": "bdc4f034-0896-4e6d-9375-fd848593a09d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Instagram Comment Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>&lt;USERNAME&gt; TOLOL!! Gak ada hubungan nya kegug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>Geblek lo tata...cowo bgt dibela2in balikan......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>Kmrn termewek2 skr lengket lg duhhh kok labil ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>negative</td>\n",
       "      <td>Intinya kalau kesel dengan ATT nya, gausah ke ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>negative</td>\n",
       "      <td>hadewwwww permpuan itu lg!!!!sakit jiwa,knp ha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id Sentiment                             Instagram Comment Text\n",
       "0   1  negative   <USERNAME> TOLOL!! Gak ada hubungan nya kegug...\n",
       "1   2  negative  Geblek lo tata...cowo bgt dibela2in balikan......\n",
       "2   3  negative  Kmrn termewek2 skr lengket lg duhhh kok labil ...\n",
       "3   4  negative  Intinya kalau kesel dengan ATT nya, gausah ke ...\n",
       "4   5  negative  hadewwwww permpuan itu lg!!!!sakit jiwa,knp ha..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eIOQcJYdodwa",
    "outputId": "ad5180ad-9eee-4e09-c645-9cc84d24a9fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    200\n",
       "negative    200\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "pCoOr7tSzPNC"
   },
   "outputs": [],
   "source": [
    "df.loc[(df.Sentiment == 'negative'),'Sentiment']=0\n",
    "df.loc[(df.Sentiment == 'positive'),'Sentiment']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fjogwFsJv887"
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(df['Instagram Comment Text'])\n",
    "y = df.Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "yn0a5Qw1uuiB"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9O8VIbfl0inp"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.values.squeeze().tolist()\n",
    "X_test = X_test.values.squeeze().tolist()\n",
    "y_train = np.array([y_train.values.squeeze().tolist()])\n",
    "y_test = np.array([y_test.values.squeeze().tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['inti', 'kesel', 'att', 'nya', 'gausah', 'anak', 'kasi', 'kembang', 'psikis', 'anak', 'depan', 'itu', 'orang', 'bener', 'bener', 'tolol', 'skrg', 'anda', 'anak', 'anak', 'dikatain', 'orang', 'benci', 'gimana', 'asa', 'benci', 'tau', 'batesnya', 'nama', 'manusia', 'gaakan', 'suka', 'haters']\n"
     ]
    }
   ],
   "source": [
    "print(process_tweet(X_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hEOOD0DD0HN9"
   },
   "source": [
    "### Build Freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell bisa dijalankan atau langsung saja import file `freqs.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mFFFA72t0GJa",
    "outputId": "c733f0af-f623-4fb2-ff84-c20ac78dfc72"
   },
   "outputs": [],
   "source": [
    "# freqs = count_tweets(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(freqs) = <class 'dict'>\n",
      "len(freqs) = 2772\n"
     ]
    }
   ],
   "source": [
    "# check output\n",
    "print(\"type(freqs) = \" + str(type(freqs)))\n",
    "print(\"len(freqs) = \" + str(len(freqs.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "## os.makedirs(name=\"data\", exist_ok=True)\n",
    "\n",
    "## with open('data/freqs.json', 'wb') as fp:\n",
    "    ##pickle.dump(freqs, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/freqs_instag.json', 'rb') as f:\n",
    "    freqs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nODmHg3LWsh"
   },
   "source": [
    "# Naive Bayes Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(freqs, X_train, y_train):\n",
    "    \n",
    "    loglikelihood = {}\n",
    "    logprior = 0\n",
    "    \n",
    "    # menghitung v, jumlah dari unik word di dalam vocabulary\n",
    "    vocab = set([pair[0] for pair in freqs.keys()]) # pakai set untuk eleminasi word yang sama\n",
    "    V = len(vocab)\n",
    "    \n",
    "    # hitung N_pos and N_neg\n",
    "    N_pos = N_neg = 0\n",
    "    for pair in freqs.keys():# bentuk ('benci', 0)\n",
    "        # jika label positif atau lebih dari 0\n",
    "        if pair[1] > 0:\n",
    "            \n",
    "            N_pos += freqs[pair]\n",
    "        \n",
    "        # else, maka label negatif\n",
    "        else:\n",
    "            \n",
    "            N_neg += freqs[pair]\n",
    "            \n",
    "    # hitung, jumlah dokumen\n",
    "    D = len(y_train[-1])\n",
    "    \n",
    "    # hitung D_pos atau jumlah dokumen yang positif\n",
    "    D_pos = sum(pos for pos in y_train[-1] if pos > 0)\n",
    "    \n",
    "    # hitung D_neg atau jumlah dokumen yang negatif\n",
    "    D_neg = D - D_pos\n",
    "    \n",
    "    # hitung logprior\n",
    "    logprior = np.log(D_pos) - np.log(D_neg)\n",
    "    \n",
    "    # untu setiap kata di dalam vocabulary\n",
    "    for word in vocab:\n",
    "        # dapatkan frekuensi positive dan negatif dalam word\n",
    "        freq_pos = lookup(freqs, word, 1.0)\n",
    "        freq_neg = lookup(freqs, word, 0.0)\n",
    "        \n",
    "        # hitung probabilitas setip word adalah positif maupun negatif\n",
    "        p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
    "        p_w_neg = (freq_neg + 1) / (N_neg + V)\n",
    "        \n",
    "        # calculate log likelihood dari kata\n",
    "        loglikelihood[word] = np.log(p_w_pos/p_w_neg)\n",
    "        \n",
    "    \n",
    "    return logprior, loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05001042057466165\n",
      "2364\n"
     ]
    }
   ],
   "source": [
    "# tes algoritma\n",
    "logprior, loglikelihood = train_naive_bayes(freqs, X_train, y_train)\n",
    "print(logprior)\n",
    "print(len(loglikelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Naive Bayes Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
    "    \n",
    "    # prosses word\n",
    "    word_l = process_tweet(tweet)\n",
    "    \n",
    "    # inisiasi probabilitas dengan 0\n",
    "    p = 0\n",
    "    \n",
    "    # tambah logprior\n",
    "    p += logprior\n",
    "    \n",
    "    \n",
    "    for word in word_l:\n",
    "        \n",
    "        # cek jika kata ada didalam loglikehood ditionary\n",
    "        if word in loglikelihood:\n",
    "            # tambahkan loglikehood dari kata tersebut ke probabilitas\n",
    "            p += loglikelihood.get(word)\n",
    "            \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output adalah 2.808931450266133\n"
     ]
    }
   ],
   "source": [
    "# test dengan tweet sendiri.\n",
    "my_tweet = 'Ganteng sekali dia.'\n",
    "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
    "print('Output adalah', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_naive_bayes(X_test, y_test, logprior, loglikelihood):\n",
    "    \n",
    "    y_hats = []\n",
    "    for tweet in X_test:\n",
    "        # jika prediksi > 0\n",
    "        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
    "            # prediksi kelas adalah 1\n",
    "            y_hat_i = 1\n",
    "        else:\n",
    "            # else, maka prediksi kelas adalah 0\n",
    "            y_hat_i = 0\n",
    "        \n",
    "        # tambahkan hasil prediksi kelas ke list y_hats\n",
    "        y_hats.append(y_hat_i)\n",
    "        \n",
    "    # error adalah rata-rata nilai absolut dari perbedaan y_hats dan y_test\n",
    "    error = np.mean(np.absolute(y_hats - y_test[-1]))\n",
    "    \n",
    "    # akurasi adalah 1 - erros\n",
    "    accuracy = 1 - error\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy = 0.9250\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes accuracy = %0.4f\" %\n",
    "      (test_naive_bayes(X_test, y_test, logprior, loglikelihood)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Beginilah pasangan suami istri yg normal.. Romantisme hanya dia dan istri yg tau. Ga perlu diumbar.. Tp dijaga. Yg suka umbar kemesraan iti kebanyakan menutupi kenyataan yg sebenarnya.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cowok macam anjing cuihhh -> -3.59\n",
      "Orang kaya malahn berperilaku sebagai mana orang sederhana -> 0.93\n",
      "Romantisme hanya dia dan istri yg tau. -> -1.05\n"
     ]
    }
   ],
   "source": [
    "for tweet in ['Cowok macam anjing cuihhh', 'Orang kaya malahn berperilaku sebagai mana orang sederhana', 'Romantisme hanya dia dan istri yg tau.']:\n",
    "    p = naive_bayes_predict(tweet, logprior, loglikelihood)\n",
    "    print(f'{tweet} -> {p:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter words by ratio dari jumlah positive dan negativ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratio(freqs, word):\n",
    "    \n",
    "    pos_neg_ratio = {'positive':0, 'negative':0, 'ratio':0.0}\n",
    "    \n",
    "    # gunakan fungsi lookup() untuk menemukan positive count untuk sebuah kata\n",
    "    pos_neg_ratio['positive'] = lookup(freqs, word, 1)\n",
    "    \n",
    "    # gunakan fungsi lookup() untuk meneukan negativ count untuk sebuah kata \n",
    "    pos_neg_ratio['negative'] = lookup(freqs, word, 0)\n",
    "    \n",
    "    # hitung rasio positif negativ count untuk word\n",
    "    pos_neg_ratio['ratio'] = (pos_neg_ratio['positive'] + 1) / (pos_neg_ratio['negative'] + 1)\n",
    "    \n",
    "    return pos_neg_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive': 0, 'negative': 1, 'ratio': 0.5}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ratio(freqs, 'belagu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_by_threshold(freqs, label, threshold):\n",
    "    \n",
    "    word_list = {}\n",
    "    \n",
    "    for key in freqs.keys():\n",
    "        word, _ = key\n",
    "        \n",
    "        # dapatkan positive atau negative ratio untuk sebuah kata\n",
    "        pos_neg_ratio = get_ratio(freqs, word)\n",
    "        \n",
    "        # jika label adalah 1 dan ratio lebih atau sama dengan threshold\n",
    "        if label == 1 and pos_neg_ratio['ratio'] >= threshold:\n",
    "            \n",
    "            # tambahkan pos_neg_ratio ke dictionary\n",
    "            word_list[word] = pos_neg_ratio\n",
    "            \n",
    "        # jika label = 0 dan pos_neg_ratio kurang dari sama dengan threshold\n",
    "        elif label == 0 and pos_neg_ratio['ratio'] <= threshold:\n",
    "            \n",
    "            word_list[word] = pos_neg_ratio\n",
    "            \n",
    "    \n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lo': {'positive': 0, 'negative': 22, 'ratio': 0.043478260869565216}}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_words_by_threshold(freqs, label=0, threshold=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cantik': {'positive': 31, 'negative': 2, 'ratio': 10.666666666666666},\n",
       " 'ganteng': {'positive': 14, 'negative': 0, 'ratio': 15.0},\n",
       " 'lancar': {'positive': 9, 'negative': 0, 'ratio': 10.0},\n",
       " 'bangga': {'positive': 10, 'negative': 0, 'ratio': 11.0},\n",
       " 'keren': {'positive': 14, 'negative': 0, 'ratio': 15.0},\n",
       " 'aurel': {'positive': 13, 'negative': 0, 'ratio': 14.0},\n",
       " 'jevin': {'positive': 12, 'negative': 0, 'ratio': 13.0}}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_words_by_threshold(freqs, label=1, threshold=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truth Predicted Tweet\n",
      "1\t0.00\tb'bagus dong cari ilmu mana suami orang didik'\n",
      "1\t0.00\tb'begini pasang suami istri yg normal romantisme istri yg tau ga umbar tp jaga yg suka umbar mesra iti banyak tutup nyata yg'\n",
      "0\t1.00\tb'anyiennnnggg suara ancur banget merdu tukang goreng'\n",
      "0\t1.00\tb'yang bilang enji ganteng bebas slingkuh bego kali ganteng pala lu bau menyan gada ganteng ganteng nya banyak gaya kek mantan bini depok nya'\n",
      "0\t1.00\tb'hahhahaha sungguh allah swt laknat wanita rebut zina suami sah oranglain'\n",
      "0\t1.00\tb'keluarga ngefans kagum lihat nya deh tapi lihat anak nya yg jelek buluk kan hermansyah kutuk yg timpa bunda kd untung allah swt ganti amora adik nya gemezzzzzzz lihat nya cocok banget anak artis class bunda kd'\n"
     ]
    }
   ],
   "source": [
    "print('Truth Predicted Tweet')\n",
    "for x, y in zip(X_test, y_test[-1]):\n",
    "    y_hat = naive_bayes_predict(x, logprior, loglikelihood)\n",
    "    if y != (np.sign(y_hat) > 0):\n",
    "        print('%d\\t%0.2f\\t%s' % (y, np.sign(y_hat) > 0, ' '.join(\n",
    "            process_tweet(x)).encode('ascii', 'ignore')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict own tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.494567761941956\n"
     ]
    }
   ],
   "source": [
    "my_tweet = 'Lo mau cari gara-gara'\n",
    "\n",
    "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
    "print(p)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "GQLN-l93iF2B",
    "LZMhPthAicoE",
    "Nh5funiyhB5G",
    "hEOOD0DD0HN9",
    "3nODmHg3LWsh",
    "TLe8VnOkMB7o",
    "GvIYq4rbRti6",
    "MQQiPZRWQ0cL",
    "YnrdXH8VTg54"
   ],
   "name": "instagram-sentimen-analisi.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:jcop_usl]",
   "language": "python",
   "name": "conda-env-jcop_usl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
