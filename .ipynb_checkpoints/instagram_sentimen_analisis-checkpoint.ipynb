{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset yang digunakan dapat didownload di: https://github.com/rizalespe/Dataset-Sentimen-Analisis-Bahasa-Indonesia atau menggunakan ***git clone*** seperti contoh dibawah ini. Folder yang di _clone_ tersimpan ke dalam folder tempat file project ini disimpan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uQhkN4DG2gXb",
    "outputId": "120f7340-a07e-4a82-b0a2-7b7de9bc2337"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/rizalespe/Dataset-Sentimen-Analisis-Bahasa-Indonesia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQLN-l93iF2B"
   },
   "source": [
    "## Install Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Requirement Package**:\n",
    "\n",
    "```\n",
    "1. nltk : https://www.nltk.org/\n",
    "2. Sastrawi: https://github.com/sastrawi/sastrawi\n",
    "3. numpy: https://numpy.org/\n",
    "4. pandas: https://pandas.pydata.org/\n",
    "5. sklearn: https://scikit-learn.org/stable/\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZMhPthAicoE"
   },
   "source": [
    "# Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install Sastrawi\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uq4zAXNPih6-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "from string import punctuation\n",
    "import os\n",
    "import json\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nh5funiyhB5G"
   },
   "source": [
    "# ```{Utils}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "UW6_M8jhgyy-"
   },
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    \n",
    "    # kumpulan stemming\n",
    "    factory_stem = StemmerFactory()\n",
    "    stemmer = factory_stem.create_stemmer()\n",
    "\n",
    "    # kumpulan stopwords\n",
    "    factory_stopwords = StopWordRemoverFactory()\n",
    "    stopword = factory_stopwords.get_stop_words() + stopwords.words('indonesian')\n",
    "  \n",
    "    # menghapus kata-kata yang tidak penting seperti @, #\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    # tokenizer word\n",
    "    tweet_tokens = word_tokenize(tweet)\n",
    "    \n",
    "    # membersihkan word\n",
    "    tweets_clean = [stemmer.stem(word) for word in tweet_tokens if (word not in stopword and word not in punctuation)]\n",
    "  \n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cKzrfhkFqV3K"
   },
   "outputs": [],
   "source": [
    "def build_freqs(tweets, ys):\n",
    "    \n",
    "    yslist = np.squeeze(ys).tolist()\n",
    "    \n",
    "    freqs = {}\n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, y)\n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            else:\n",
    "                freqs[pair] = 1\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hmi7dW_Ttakm"
   },
   "source": [
    "# Processing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLVQNDWptjJF"
   },
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OnmXDdv7ia_u"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Dataset-Sentimen-Analisis-Bahasa-Indonesia/dataset_komentar_instagram_cyberbullying.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "upLfzDUrihHG",
    "outputId": "bdc4f034-0896-4e6d-9375-fd848593a09d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Instagram Comment Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>&lt;USERNAME&gt; TOLOL!! Gak ada hubungan nya kegug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>Geblek lo tata...cowo bgt dibela2in balikan......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>Kmrn termewek2 skr lengket lg duhhh kok labil ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>negative</td>\n",
       "      <td>Intinya kalau kesel dengan ATT nya, gausah ke ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>negative</td>\n",
       "      <td>hadewwwww permpuan itu lg!!!!sakit jiwa,knp ha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id Sentiment                             Instagram Comment Text\n",
       "0   1  negative   <USERNAME> TOLOL!! Gak ada hubungan nya kegug...\n",
       "1   2  negative  Geblek lo tata...cowo bgt dibela2in balikan......\n",
       "2   3  negative  Kmrn termewek2 skr lengket lg duhhh kok labil ...\n",
       "3   4  negative  Intinya kalau kesel dengan ATT nya, gausah ke ...\n",
       "4   5  negative  hadewwwww permpuan itu lg!!!!sakit jiwa,knp ha..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eIOQcJYdodwa",
    "outputId": "ad5180ad-9eee-4e09-c645-9cc84d24a9fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    200\n",
       "negative    200\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "pCoOr7tSzPNC"
   },
   "outputs": [],
   "source": [
    "df.loc[(df.Sentiment == 'negative'),'Sentiment']=0\n",
    "df.loc[(df.Sentiment == 'positive'),'Sentiment']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fjogwFsJv887"
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(df['Instagram Comment Text'])\n",
    "y = df.Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "yn0a5Qw1uuiB"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9O8VIbfl0inp"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.values.squeeze().tolist()\n",
    "X_test = X_test.values.squeeze().tolist()\n",
    "y_train = np.array([y_train.values.squeeze().tolist()])\n",
    "y_test = np.array([y_test.values.squeeze().tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Intinya kalau kesel dengan ATT nya, gausah ke anaknya juga. Kasian buat perkembangan psikis anak kedepannya. Itu orang bener bener tolol, skrg seandainya dia punya anak, terus anaknya dikatain sama orang yang benci sama dia, gimana perasaan dia ? Benci sama seseorang boleh, tapi harus tau batesnya ?? toh namanya manusia, gaakan semua jadi penyuka, pasti ada haters ??'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hEOOD0DD0HN9"
   },
   "source": [
    "### Build Freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell bisa dijalankan atau langsung saja import file `freqs.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mFFFA72t0GJa",
    "outputId": "c733f0af-f623-4fb2-ff84-c20ac78dfc72"
   },
   "outputs": [],
   "source": [
    "# freqs = build_freqs(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check output\n",
    "print(\"type(freqs) = \" + str(type(freqs)))\n",
    "print(\"len(freqs) = \" + str(len(freqs.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(name=\"data\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pKzNnojlKVml"
   },
   "outputs": [],
   "source": [
    "#with open('data/freqs_utf8.json', 'wb') as fp:\n",
    "    #pickle.dump(freqs, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/freqs_utf8.json', 'rb') as f:\n",
    "    freqs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contoh sample positive tweet: \n",
      "  <USERNAME> yaa ampun ini upil naruto,, kata2nya makin mencerminkan klo dia yg sebenarnya pecun.????pengalaman banget lu yaa. Kesian. Gua jadi prihatin. Mati aj deh lu. Ngotorin dunia manusia macam kau.??\n",
      "\n",
      "Contoh yang sudah di proses: \n",
      " ['username', 'yaa', 'ampun', 'upil', 'naruto', 'kata2nya', 'cermin', 'klo', 'yg', 'pecun', 'alam', 'banget', 'lu', 'yaa', 'kesi', 'gua', 'prihatin', 'mati', 'aj', 'deh', 'lu', 'ngotorin', 'dunia', 'manusia', 'kau']\n"
     ]
    }
   ],
   "source": [
    "# test proses fungsi\n",
    "print('Contoh sample positive tweet: \\n', X_train[1])\n",
    "print('\\nContoh yang sudah di proses: \\n', process_tweet(X_train[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nODmHg3LWsh"
   },
   "source": [
    "# Logistic Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLe8VnOkMB7o"
   },
   "source": [
    "### Sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "NGuvWFUBLbVI"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \n",
    "    h = 1/(1+np.exp(-z))\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS!\n"
     ]
    }
   ],
   "source": [
    "# Testing fungsi \n",
    "if (sigmoid(0) == 0.5):\n",
    "    print('SUCCESS!')\n",
    "else:\n",
    "    print('Oops!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvIYq4rbRti6"
   },
   "source": [
    "### Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "yT9aAF-IR0qJ"
   },
   "outputs": [],
   "source": [
    "def linreg_cost_func(x, y, m, h):\n",
    "    \n",
    "    # calculate fungsi cost\n",
    "    J = -1/m * (np.dot(y.T, np.log(h)) + (np.dot((1-y).T, np.log(1-h))))\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQQiPZRWQ0cL"
   },
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Dpw9tdycQ2ta"
   },
   "outputs": [],
   "source": [
    "def gradientDescent(x, y, theta, alpha, num_iters):\n",
    "    \n",
    "    m = x.shape[0]\n",
    "    \n",
    "    for i in range(0, num_iters):\n",
    "        \n",
    "        # feed forward\n",
    "        z = np.dot(x, theta)\n",
    "        h = sigmoid(z)\n",
    "\n",
    "        # hitung cost\n",
    "        J = linreg_cost_func(x, y, m, h)\n",
    "        \n",
    "        # update weight\n",
    "        theta = theta - ((alpha/m) * (np.dot(x.T, h-y)))\n",
    "        \n",
    "        print(f'\\rIterasi: {i+1}/{num_iters}', end='')\n",
    "\n",
    "    J = float(J)\n",
    "    return J, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterasi: 700/700\n",
      "Cost setelah training adalah 0.67094970.\n"
     ]
    }
   ],
   "source": [
    "# testing fungsi\n",
    "np.random.seed(1)\n",
    "tmp_X = np.append(np.ones((10, 1)), np.random.rand(10, 2) * 2000, axis=1)\n",
    "tmp_Y = (np.random.rand(10, 1) > 0.35).astype(float)\n",
    "\n",
    "\n",
    "# Apply gradient descent\n",
    "tmp_J, tmp_theta = gradientDescent(tmp_X, tmp_Y, np.zeros((3, 1)), 1e-8, 700)\n",
    "print(f\"\\nCost setelah training adalah {tmp_J:.8f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnrdXH8VTg54"
   },
   "source": [
    "# Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "wmCN9UdyTiyZ"
   },
   "outputs": [],
   "source": [
    "def extract_features(tweet, freqs):\n",
    "    \n",
    "    # process_tweet tokenizes, stems, dan hapus stopwords\n",
    "    word_l = process_tweet(tweet)\n",
    "    \n",
    "    # inisiasi x dengan nilai 0 dalam bentuk matriks 1x3 \n",
    "    x = np.zeros((1, 3)) \n",
    "    \n",
    "    # inisiasi bias dengan 1\n",
    "    x[0,0] = 1 \n",
    "        \n",
    "    for word in word_l:\n",
    "        \n",
    "        # increment the word count for the positive label 1\n",
    "        x[0,1] += freqs.get((word, 1.0),0)\n",
    "        \n",
    "        # increment the word count for the negative label 0\n",
    "        x[0,2] += freqs.get((word, 0.0),0)\n",
    "        \n",
    "    assert(x.shape == (1, 3))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1. 528. 659.]]\n"
     ]
    }
   ],
   "source": [
    "# test fungsi\n",
    "tmp1 = extract_features(X_train[5], freqs)\n",
    "print(tmp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "tmp2 = extract_features('blorb bleeeeb bloooob', freqs)\n",
    "print(tmp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QMBt0LsN4pk"
   },
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "kIpB37KdN7Q3",
    "outputId": "084cc479-5d68-4573-c3d8-54c9260404d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract: 320/320 "
     ]
    }
   ],
   "source": [
    "X = np.zeros((len(X_train), 3))\n",
    "for i in range(len(X_train)):\n",
    "    X[i, :]= extract_features(X_train[i], freqs)\n",
    "    print(f'\\rExtract: {i+1}/{len(X_train)}', end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = y_train.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(name=\"model\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'input': X,\n",
    "    'output':Y,\n",
    "    'alpha':1e-5,\n",
    "    'theta':np.zeros((3, 1)),\n",
    "    'num_iters':3000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model/model3.pickle', 'wb') as fp:\n",
    "    pickle.dump(config, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "l6ol1Q1UOAam",
    "outputId": "403e3ef7-aa0a-48f7-a25f-8a0eee6bde98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterasi: 3000/3000\n",
      "Cost setelah training adalah 0.20258613.\n"
     ]
    }
   ],
   "source": [
    "# Apply gradient descent\n",
    "J, theta = gradientDescent(config['input'], config['output'], config['theta'], config['alpha'], config['num_iters'])\n",
    "print(f\"\\nCost setelah training adalah {J:.8f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00042596],\n",
       "       [ 0.06527373],\n",
       "       [-0.05839513]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save bobot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model/theta03.wt', 'wb') as fp:\n",
    "    pickle.dump(theta, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tI98hOCGOAt2"
   },
   "source": [
    "# Test Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load bobot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model/theta03.wt', 'rb') as f:\n",
    "    theta = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00042596],\n",
       "       [ 0.06527373],\n",
       "       [-0.05839513]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "xvScaBnPOA3m"
   },
   "outputs": [],
   "source": [
    "def predict_tweet(tweet, freqs, theta):\n",
    "\n",
    "    # extract features dari tweet dan simpan di dalam x\n",
    "    x = extract_features(tweet, freqs)\n",
    "    \n",
    "    # buat prediksi menggunakan x dan bobot (theta)\n",
    "    y_pred = sigmoid(np.dot(x, theta))\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bangsat lu -> 0.321922\n",
      "mantap sihh ini -> 0.516419\n",
      "apaan lo? -> 0.216825\n",
      "congrats yah k -> 0.443671\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to test your function\n",
    "for tweet in ['bangsat lu', 'mantap sihh ini', 'apaan lo?', 'congrats yah k']:\n",
    "    print( '%s -> %f' % (tweet, predict_tweet(tweet, freqs, theta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.48551176]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feel free to check the sentiment of your own tweet below\n",
    "my_tweet = 'busuk'\n",
    "predict_tweet(my_tweet, freqs, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check peformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
    "\n",
    "    \n",
    "    # the list for storing predictions\n",
    "    y_hat = []\n",
    "    \n",
    "    for tweet in test_x:\n",
    "        \n",
    "        y_pred = predict_tweet(tweet, freqs, theta)\n",
    "        \n",
    "        if y_pred > 0.5:\n",
    "            # append 1.0 to the list\n",
    "            y_hat.append(1)\n",
    "        else:\n",
    "            # append 0 to the list\n",
    "            y_hat.append(0)\n",
    "\n",
    "    accuracy = (y_hat==np.squeeze(test_y)).sum()/len(test_x)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression model's accuracy = 0.9000\n"
     ]
    }
   ],
   "source": [
    "tmp_accuracy = test_logistic_regression(X_test, y_test, freqs, theta)\n",
    "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Predicted Tweet\n",
      "tweet: Anyiennnnggg.. Suaranya ancur banget, lebih merdu tukang gorengan\n",
      "proses tweet: ['anyiennnnggg', 'suara', 'ancur', 'banget', 'merdu', 'tukang', 'goreng']\n",
      "0\t0.84613034\tb'anyiennnnggg suara ancur banget merdu tukang goreng'\n",
      "\n",
      "\n",
      "tweet: Berasa kaya mau manggung kk  <USERNAME> ?? semoga lancar dan cepat punya momongan biar Ada Princess Kecil MoMo atau gak prince ??\n",
      "proses tweet: ['asa', 'kaya', 'manggung', 'kk', 'username', 'moga', 'lancar', 'cepat', 'momong', 'biar', 'ada', 'princess', 'kecil', 'momo', 'gak', 'prince']\n",
      "1\t0.21405383\tb'asa kaya manggung kk username moga lancar cepat momong biar ada princess kecil momo gak prince'\n",
      "\n",
      "\n",
      "tweet: Noraaak abiiis... Baru kebeli emas az pamer aplg mainan berlian ciiinnnnn.... ??\n",
      "proses tweet: ['noraaak', 'abiiis', '', 'baru', 'kebel', 'emas', 'az', 'pamer', 'aplg', 'main', 'berlian', 'ciiinnnnn', '']\n",
      "0\t0.80596540\tb'noraaak abiiis  baru kebel emas az pamer aplg main berlian ciiinnnnn '\n",
      "\n",
      "\n",
      "tweet: Hahhahaha dan sesungguhnya allah swt melaknat wanita perebut dan pezina dengan suami sah oranglain ????????\n",
      "proses tweet: ['hahhahaha', 'sungguh', 'allah', 'swt', 'laknat', 'wanita', 'rebut', 'zina', 'suami', 'sah', 'oranglain']\n",
      "0\t0.72363294\tb'hahhahaha sungguh allah swt laknat wanita rebut zina suami sah oranglain'\n",
      "\n",
      "\n",
      "tweet: Before and after pasca oplas ga ngefek ya.. tetep jelek n serem mukanya..ahahahhaha..\n",
      "proses tweet: ['before', 'and', 'after', 'pasca', 'oplas', 'ga', 'ngefek', 'ya', 'tetep', 'jelek', 'n', 'rem', 'muka ahahahhaha']\n",
      "0\t0.68509143\tb'before and after pasca oplas ga ngefek ya tetep jelek n rem muka ahahahhaha'\n",
      "\n",
      "\n",
      "tweet: Wah itu Kerdus mulutnya busuk...jangan jangan sampah makanannya\n",
      "proses tweet: ['wah', 'kerdus', 'mulut', 'busuk', '', 'sampah', 'makan']\n",
      "0\t0.50372815\tb'wah kerdus mulut busuk  sampah makan'\n",
      "\n",
      "\n",
      "tweet: bencong rombeng.menjijikan!!!gay jg.\n",
      "proses tweet: ['bencong', 'rombeng jijik', 'gay', 'jg']\n",
      "0\t0.50442374\tb'bencong rombeng jijik gay jg'\n",
      "\n",
      "\n",
      "tweet: Dari awal percaya gimik Raffi gigi dn att...sehingga att peserta keluarga di hujad dn anak ny...di hujad ...semakin kesini penonton semakin di bodohin?? semakin publik tertuju mereka ??????yg kasihan para penghujad ....ga apa\" lah ayu ga ada di pesbukers biar dia ngeluarkn album\" baru single baru setelah para artis halan\" keluar nengri gimik\" apa lagi yoo\n",
      "proses tweet: ['dari', 'percaya', 'gimik', 'raffi', 'gigi', 'dn', 'att', '', 'att', 'serta', 'keluarga', 'hujad', 'dn', 'anak', 'ny', '', 'hujad', '', 'kesini', 'tonton', 'bodohin', 'publik', 'yg', 'kasihan', 'penghujad', '', 'ga', '', 'ayu', 'ga', 'pesbukers', 'biar', 'ngeluarkn', 'album', '', 'single', 'artis', 'hal', '', 'nengri', 'gimik', '', 'yoo']\n",
      "0\t0.99839838\tb'dari percaya gimik raffi gigi dn att  att serta keluarga hujad dn anak ny  hujad  kesini tonton bodohin publik yg kasihan penghujad  ga  ayu ga pesbukers biar ngeluarkn album  single artis hal  nengri gimik  yoo'\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Some error analysis \n",
    "print('Label Predicted Tweet')\n",
    "for x,y in zip(X_test,y_test):\n",
    "    y_hat = predict_tweet(x, freqs, theta)\n",
    "    if np.abs(y - (y_hat > 0.5)) > 0:\n",
    "        print('tweet:', x)\n",
    "        print('proses tweet:', process_tweet(x))\n",
    "        print('%d\\t%0.8f\\t%s' % (y, y_hat, ' '.join(process_tweet(x)).encode('ascii', 'ignore')))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict own tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lo']\n",
      "[[0.21682477]]\n",
      "Negative sentiment\n"
     ]
    }
   ],
   "source": [
    "# coba predik\n",
    "my_tweet = \"\"\"\n",
    "apaan lo\n",
    "\"\"\"\n",
    "print(process_tweet(my_tweet))\n",
    "y_hat = predict_tweet(my_tweet, freqs, theta)\n",
    "print(y_hat)\n",
    "if y_hat > 0.5:\n",
    "    print('Positive sentiment')\n",
    "else: \n",
    "    print('Negative sentiment')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "GQLN-l93iF2B",
    "LZMhPthAicoE",
    "Nh5funiyhB5G",
    "hEOOD0DD0HN9",
    "3nODmHg3LWsh",
    "TLe8VnOkMB7o",
    "GvIYq4rbRti6",
    "MQQiPZRWQ0cL",
    "YnrdXH8VTg54"
   ],
   "name": "instagram-sentimen-analisi.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:jcop_usl]",
   "language": "python",
   "name": "conda-env-jcop_usl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
