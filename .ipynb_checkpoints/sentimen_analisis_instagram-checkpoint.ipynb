{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset yang digunakan dapat didownload di: https://github.com/rizalespe/Dataset-Sentimen-Analisis-Bahasa-Indonesia atau menggunakan ***git clone*** seperti contoh dibawah ini. Folder yang di _clone_ tersimpan ke dalam folder tempat file project ini disimpan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uQhkN4DG2gXb",
    "outputId": "120f7340-a07e-4a82-b0a2-7b7de9bc2337"
   },
   "outputs": [],
   "source": [
    "#!git clone https://github.com/rizalespe/Dataset-Sentimen-Analisis-Bahasa-Indonesia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQLN-l93iF2B"
   },
   "source": [
    "## Install Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Requirement Package**:\n",
    "\n",
    "```\n",
    "1. nltk : https://www.nltk.org/\n",
    "2. Sastrawi: https://github.com/sastrawi/sastrawi\n",
    "3. numpy: https://numpy.org/\n",
    "4. pandas: https://pandas.pydata.org/\n",
    "5. sklearn: https://scikit-learn.org/stable/\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZMhPthAicoE"
   },
   "source": [
    "# Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install Sastrawi\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uq4zAXNPih6-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "from string import punctuation\n",
    "import os\n",
    "import json\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nh5funiyhB5G"
   },
   "source": [
    "# ```{Utils}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UW6_M8jhgyy-"
   },
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    \n",
    "    # kumpulan stemming\n",
    "    factory_stem = StemmerFactory()\n",
    "    stemmer = factory_stem.create_stemmer()\n",
    "\n",
    "    # kumpulan stopwords\n",
    "    factory_stopwords = StopWordRemoverFactory()\n",
    "    stopword = factory_stopwords.get_stop_words() + stopwords.words('indonesian')\n",
    "  \n",
    "    # menghapus kata-kata yang tidak penting seperti @, #\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    tweet = re.sub(r'<USERNAME>', '', tweet)\n",
    "    \n",
    "    # tokenizer word\n",
    "    tweet_tokens = word_tokenize(tweet)\n",
    "    \n",
    "    # membersihkan word\n",
    "    tweets_clean = [stemmer.stem(word) for word in tweet_tokens if (word not in stopword and word not in punctuation)]\n",
    "  \n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cKzrfhkFqV3K"
   },
   "outputs": [],
   "source": [
    "def build_freqs(tweets, ys):\n",
    "    \n",
    "    yslist = np.squeeze(ys).tolist()\n",
    "    \n",
    "    freqs = {}\n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, y)\n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            else:\n",
    "                freqs[pair] = 1\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hmi7dW_Ttakm"
   },
   "source": [
    "# Processing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLVQNDWptjJF"
   },
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "OnmXDdv7ia_u"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/dataset_komentar_instagram_cyberbullying.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "upLfzDUrihHG",
    "outputId": "bdc4f034-0896-4e6d-9375-fd848593a09d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Instagram Comment Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>&lt;USERNAME&gt; TOLOL!! Gak ada hubungan nya kegug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>Geblek lo tata...cowo bgt dibela2in balikan......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>Kmrn termewek2 skr lengket lg duhhh kok labil ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>negative</td>\n",
       "      <td>Intinya kalau kesel dengan ATT nya, gausah ke ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>negative</td>\n",
       "      <td>hadewwwww permpuan itu lg!!!!sakit jiwa,knp ha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id Sentiment                             Instagram Comment Text\n",
       "0   1  negative   <USERNAME> TOLOL!! Gak ada hubungan nya kegug...\n",
       "1   2  negative  Geblek lo tata...cowo bgt dibela2in balikan......\n",
       "2   3  negative  Kmrn termewek2 skr lengket lg duhhh kok labil ...\n",
       "3   4  negative  Intinya kalau kesel dengan ATT nya, gausah ke ...\n",
       "4   5  negative  hadewwwww permpuan itu lg!!!!sakit jiwa,knp ha..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eIOQcJYdodwa",
    "outputId": "ad5180ad-9eee-4e09-c645-9cc84d24a9fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    200\n",
       "positive    200\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "pCoOr7tSzPNC"
   },
   "outputs": [],
   "source": [
    "df.loc[(df.Sentiment == 'negative'),'Sentiment']=0\n",
    "df.loc[(df.Sentiment == 'positive'),'Sentiment']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "fjogwFsJv887"
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(df['Instagram Comment Text'])\n",
    "y = df.Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "yn0a5Qw1uuiB"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "9O8VIbfl0inp"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.values.squeeze().tolist()\n",
    "X_test = X_test.values.squeeze().tolist()\n",
    "y_train = np.array([y_train.values.squeeze().tolist()])\n",
    "y_test = np.array([y_test.values.squeeze().tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Intinya kalau kesel dengan ATT nya, gausah ke anaknya juga. Kasian buat perkembangan psikis anak kedepannya. Itu orang bener bener tolol, skrg seandainya dia punya anak, terus anaknya dikatain sama orang yang benci sama dia, gimana perasaan dia ? Benci sama seseorang boleh, tapi harus tau batesnya ?? toh namanya manusia, gaakan semua jadi penyuka, pasti ada haters ??'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hEOOD0DD0HN9"
   },
   "source": [
    "### Build Freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell bisa dijalankan atau langsung saja import file `freqs.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mFFFA72t0GJa",
    "outputId": "c733f0af-f623-4fb2-ff84-c20ac78dfc72"
   },
   "outputs": [],
   "source": [
    "# freqs = build_freqs(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(freqs) = <class 'dict'>\n",
      "len(freqs) = 2772\n"
     ]
    }
   ],
   "source": [
    "# check output\n",
    "print(\"type(freqs) = \" + str(type(freqs)))\n",
    "print(\"len(freqs) = \" + str(len(freqs.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(name=\"data\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "pKzNnojlKVml"
   },
   "outputs": [],
   "source": [
    "#with open('data/freqs.json', 'wb') as fp:\n",
    "    #pickle.dump(freqs, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/freqs.json', 'rb') as f:\n",
    "    freqs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contoh sample positive tweet: \n",
      "  <USERNAME> yaa ampun ini upil naruto,, kata2nya makin mencerminkan klo dia yg sebenarnya pecun.????pengalaman banget lu yaa. Kesian. Gua jadi prihatin. Mati aj deh lu. Ngotorin dunia manusia macam kau.??\n",
      "\n",
      "Contoh yang sudah di proses: \n",
      " ['yaa', 'ampun', 'upil', 'naruto', 'kata2nya', 'cermin', 'klo', 'yg', 'pecun', 'alam', 'banget', 'lu', 'yaa', 'kesi', 'gua', 'prihatin', 'mati', 'aj', 'deh', 'lu', 'ngotorin', 'dunia', 'manusia', 'kau']\n"
     ]
    }
   ],
   "source": [
    "# test proses fungsi\n",
    "print('Contoh sample positive tweet: \\n', X_train[1])\n",
    "print('\\nContoh yang sudah di proses: \\n', process_tweet(X_train[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nODmHg3LWsh"
   },
   "source": [
    "# Logistic Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLe8VnOkMB7o"
   },
   "source": [
    "### Sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "NGuvWFUBLbVI"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \n",
    "    h = 1/(1+np.exp(-z))\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS!\n"
     ]
    }
   ],
   "source": [
    "# Testing fungsi \n",
    "if (sigmoid(0) == 0.5):\n",
    "    print('SUCCESS!')\n",
    "else:\n",
    "    print('Oops!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvIYq4rbRti6"
   },
   "source": [
    "### Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "yT9aAF-IR0qJ"
   },
   "outputs": [],
   "source": [
    "def linreg_cost_func(x, y, m, h):\n",
    "    \n",
    "    # calculate fungsi cost\n",
    "    J = -1/m * (np.dot(y.T, np.log(h)) + (np.dot((1-y).T, np.log(1-h))))\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQQiPZRWQ0cL"
   },
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Dpw9tdycQ2ta"
   },
   "outputs": [],
   "source": [
    "def gradientDescent(x, y, theta, alpha, num_iters):\n",
    "    \n",
    "    m = x.shape[0]\n",
    "    \n",
    "    for i in range(0, num_iters):\n",
    "        \n",
    "        # feed forward\n",
    "        z = np.dot(x, theta)\n",
    "        h = sigmoid(z)\n",
    "\n",
    "        # hitung cost\n",
    "        J = linreg_cost_func(x, y, m, h)\n",
    "        \n",
    "        # update weight\n",
    "        theta = theta - ((alpha/m) * (np.dot(x.T, h-y)))\n",
    "        \n",
    "        print(f'\\rIterasi: {i+1}/{num_iters}', end='')\n",
    "\n",
    "    J = float(J)\n",
    "    return J, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterasi: 2000/2000\n",
      "Cost setelah training adalah 0.65476423.\n"
     ]
    }
   ],
   "source": [
    "# testing fungsi\n",
    "np.random.seed(1)\n",
    "tmp_X = np.append(np.ones((10, 1)), np.random.rand(10, 2) * 2000, axis=1)\n",
    "tmp_Y = (np.random.rand(10, 1) > 0.35).astype(float)\n",
    "\n",
    "\n",
    "# Apply gradient descent\n",
    "tmp_J, tmp_theta = gradientDescent(tmp_X, tmp_Y, np.zeros((3, 1)), 1e-8, 2000)\n",
    "print(f\"\\nCost setelah training adalah {tmp_J:.8f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnrdXH8VTg54"
   },
   "source": [
    "# Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "wmCN9UdyTiyZ"
   },
   "outputs": [],
   "source": [
    "def extract_features(tweet, freqs):\n",
    "    \n",
    "    # process_tweet tokenizes, stems, dan hapus stopwords\n",
    "    word_l = process_tweet(tweet)\n",
    "    \n",
    "    # inisiasi x dengan nilai 0 dalam bentuk matriks 1x3 \n",
    "    x = np.zeros((1, 3)) \n",
    "    \n",
    "    # inisiasi bias dengan 1\n",
    "    x[0,0] = 1 \n",
    "        \n",
    "    for word in word_l:\n",
    "        \n",
    "        # increment the word count for the positive label 1\n",
    "        x[0,1] += freqs.get((word, 1.0),0)\n",
    "        \n",
    "        # increment the word count for the negative label 0\n",
    "        x[0,2] += freqs.get((word, 0.0),0)\n",
    "        \n",
    "    assert(x.shape == (1, 3))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1. 502. 605.]]\n"
     ]
    }
   ],
   "source": [
    "# test fungsi\n",
    "tmp1 = extract_features(X_train[5], freqs)\n",
    "print(tmp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1. 13.  8.]]\n"
     ]
    }
   ],
   "source": [
    "tmp2 = extract_features('bajingan klo kamu', freqs)\n",
    "print(tmp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QMBt0LsN4pk"
   },
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "kIpB37KdN7Q3",
    "outputId": "084cc479-5d68-4573-c3d8-54c9260404d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract: 320/320 "
     ]
    }
   ],
   "source": [
    "X = np.zeros((len(X_train), 3))\n",
    "for i in range(len(X_train)):\n",
    "    X[i, :]= extract_features(X_train[i], freqs)\n",
    "    print(f'\\rExtract: {i+1}/{len(X_train)}', end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = y_train.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(name=\"model\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'input': X,\n",
    "    'output':Y,\n",
    "    'alpha':1e-8,\n",
    "    'theta':np.zeros((3, 1)),\n",
    "    'num_iters':2000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model/model_best_linreg.pickle', 'wb') as fp:\n",
    "    pickle.dump(config, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "l6ol1Q1UOAam",
    "outputId": "403e3ef7-aa0a-48f7-a25f-8a0eee6bde98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterasi: 2000/2000\n",
      "Cost setelah training adalah 0.68604902.\n"
     ]
    }
   ],
   "source": [
    "# Apply gradient descent\n",
    "J, theta = gradientDescent(config['input'], config['output'], config['theta'], config['alpha'], config['num_iters'])\n",
    "print(f\"\\nCost setelah training adalah {J:.8f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.20377889e-07],\n",
       "       [ 2.02033706e-04],\n",
       "       [-3.17695416e-04]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save bobot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model/theta_best_linreg.wt', 'wb') as fp:\n",
    "    pickle.dump(theta, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tI98hOCGOAt2"
   },
   "source": [
    "# Test Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load bobot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model/theta_best_linreg.wt', 'rb') as f:\n",
    "    theta = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.20377889e-07],\n",
       "       [ 2.02033706e-04],\n",
       "       [-3.17695416e-04]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "xvScaBnPOA3m"
   },
   "outputs": [],
   "source": [
    "def predict_tweet(tweet, freqs, theta):\n",
    "\n",
    "    # extract features dari tweet dan simpan di dalam x\n",
    "    x = extract_features(tweet, freqs)\n",
    "    \n",
    "    # buat prediksi menggunakan x dan bobot (theta)\n",
    "    y_pred = sigmoid(np.dot(x, theta))\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bangsat lu -> 0.498910\n",
      "mantap sihh ini -> 0.500051\n",
      "apaan lo? -> 0.498253\n",
      "congrats yah k -> 0.499653\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to test your function\n",
    "for tweet in ['bangsat lu', 'mantap sihh ini', 'apaan lo?', 'congrats yah k']:\n",
    "    print( '%s -> %f' % (tweet, predict_tweet(tweet, freqs, theta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49989174]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feel free to check the sentiment of your own tweet below\n",
    "my_tweet = 'bau busuk barangnya'\n",
    "predict_tweet(my_tweet, freqs, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check peformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
    "\n",
    "    \n",
    "    # the list for storing predictions\n",
    "    y_hat = []\n",
    "    \n",
    "    for tweet in test_x:\n",
    "        \n",
    "        y_pred = predict_tweet(tweet, freqs, theta)\n",
    "        \n",
    "        if y_pred > 0.5:\n",
    "            # append 1.0 to the list\n",
    "            y_hat.append(1)\n",
    "        else:\n",
    "            # append 0 to the list\n",
    "            y_hat.append(0)\n",
    "\n",
    "    accuracy = (y_hat==np.squeeze(test_y)).sum()/len(test_x)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression model's accuracy = 0.7250\n"
     ]
    }
   ],
   "source": [
    "tmp_accuracy = test_logistic_regression(X_test, y_test, freqs, theta)\n",
    "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Predicted Tweet\n",
      "tweet: Bagus dong.. mencari ilmu kapanpun dimanapun.. itu karena suaminya orang berpendidikan ????????????\n",
      "proses tweet: ['bagus', 'dong', 'cari', 'ilmu', 'mana', 'suami', 'orang', 'didik']\n",
      "1\t0.49843903\tb'bagus dong cari ilmu mana suami orang didik'\n",
      "\n",
      "\n",
      "tweet: Beginilah pasangan suami istri yg normal.. Romantisme hanya dia dan istri yg tau. Ga perlu diumbar.. Tp dijaga. Yg suka umbar kemesraan iti kebanyakan menutupi kenyataan yg sebenarnya.\n",
      "proses tweet: ['begini', 'pasang', 'suami', 'istri', 'yg', 'normal', 'romantisme', 'istri', 'yg', 'tau', 'ga', 'umbar', 'tp', 'jaga', 'yg', 'suka', 'umbar', 'mesra', 'iti', 'banyak', 'tutup', 'nyata', 'yg']\n",
      "1\t0.48484908\tb'begini pasang suami istri yg normal romantisme istri yg tau ga umbar tp jaga yg suka umbar mesra iti banyak tutup nyata yg'\n",
      "\n",
      "\n",
      "tweet: Cantik, polos, apa adanya tapi keliatan berbakat. Semoga kalau udah tenar gek jadi selebgram, kalian ga kaya2 si mba Novilda ya dek yg kehilangan arah. Tetaplah begini,jangan cepat tumbuh dewasa dan terus asahlah bakat kalian, Allah bless you both.\n",
      "proses tweet: ['cantik', 'polos', 'liat', 'bakat', 'moga', 'udah', 'tenar', 'gek', 'selebgram', 'ga', 'kaya2', 'si', 'mba', 'novilda', 'dek', 'yg', 'hilang', 'arah', 'tetap', 'cepat', 'tumbuh', 'dewasa', 'asah', 'bakat', 'allah', 'bless', 'you', 'both']\n",
      "1\t0.49610710\tb'cantik polos liat bakat moga udah tenar gek selebgram ga kaya2 si mba novilda dek yg hilang arah tetap cepat tumbuh dewasa asah bakat allah bless you both'\n",
      "\n",
      "\n",
      "tweet: Anyiennnnggg.. Suaranya ancur banget, lebih merdu tukang gorengan\n",
      "proses tweet: ['anyiennnnggg', 'suara', 'ancur', 'banget', 'merdu', 'tukang', 'goreng']\n",
      "0\t0.50063397\tb'anyiennnnggg suara ancur banget merdu tukang goreng'\n",
      "\n",
      "\n",
      "tweet: Berasa kaya mau manggung kk  <USERNAME> ?? semoga lancar dan cepat punya momongan biar Ada Princess Kecil MoMo atau gak prince ??\n",
      "proses tweet: ['asa', 'kaya', 'manggung', 'kk', 'moga', 'lancar', 'cepat', 'momong', 'biar', 'ada', 'princess', 'kecil', 'momo', 'gak', 'prince']\n",
      "1\t0.49772347\tb'asa kaya manggung kk moga lancar cepat momong biar ada princess kecil momo gak prince'\n",
      "\n",
      "\n",
      "tweet: Wajah2 anak skul bgt yaakk mase alami bgt suka liatnya ???? lagu ma suara keren palg yg pake bando suaranya mantap ??????\n",
      "proses tweet: ['wajah2', 'anak', 'skul', 'bgt', 'yaakk', 'mase', 'alami', 'bgt', 'suka', 'liat', 'lagu', 'ma', 'suara', 'keren', 'palg', 'yg', 'pake', 'bando', 'suara', 'mantap']\n",
      "1\t0.49724133\tb'wajah2 anak skul bgt yaakk mase alami bgt suka liat lagu ma suara keren palg yg pake bando suara mantap'\n",
      "\n",
      "\n",
      "tweet: Semoga di berikan kesehatan dan rezeki yg berlimpaah pak ??\n",
      "proses tweet: ['moga', 'sehat', 'rezeki', 'yg', 'berlimpaah']\n",
      "1\t0.49831380\tb'moga sehat rezeki yg berlimpaah'\n",
      "\n",
      "\n",
      "tweet: Salut sekali Banting tulang demi menghidupi keluarga.yg duduk di bangku empuk,pakai AC Malah merampok.yg milih rakyat jelata lagi\n",
      "proses tweet: ['salut', 'banting', 'tulang', 'hidup', 'keluarga yg', 'duduk', 'bangku', 'empuk', 'pakai', 'ac', 'malah', 'rampok yg', 'milih', 'rakyat', 'jelata']\n",
      "1\t0.49965310\tb'salut banting tulang hidup keluarga yg duduk bangku empuk pakai ac malah rampok yg milih rakyat jelata'\n",
      "\n",
      "\n",
      "tweet: Kereeen nih...enak bgt didengernya...gak bosen puter ulang2...daripada yg lipsing2 gajelas joget2 gtu...\n",
      "proses tweet: ['kereeen', 'nih', '', 'enak', 'bgt', 'didengernya', '', 'gak', 'bosen', 'puter', 'ulang2', '', 'yg', 'lipsing2', 'gajelas', 'joget2', 'gtu', '']\n",
      "1\t0.48603831\tb'kereeen nih  enak bgt didengernya  gak bosen puter ulang2  yg lipsing2 gajelas joget2 gtu '\n",
      "\n",
      "\n",
      "tweet: Apaoun pekerjaannya yg penting halal u tuk menafkahi keluarga ... Semangat pak\n",
      "proses tweet: ['apaoun', 'kerja', 'yg', 'halal', 'u', 'tuk', 'nafkah', 'keluarga', '', 'semangat']\n",
      "1\t0.49597905\tb'apaoun kerja yg halal u tuk nafkah keluarga  semangat'\n",
      "\n",
      "\n",
      "tweet: Subhanallah tergetar hati dengar suara nya\n",
      "proses tweet: ['subhanallah', 'getar', 'hati', 'dengar', 'suara', 'nya']\n",
      "1\t0.49863374\tb'subhanallah getar hati dengar suara nya'\n",
      "\n",
      "\n",
      "tweet: Bangga sama suami yg selalu ingat istri disela waktunya, apapun profesimu... Kaulah juaranya. Sehat terus pak gojek, semoga lacar rejekimu. Amiiin....\n",
      "proses tweet: ['bangga', 'suami', 'yg', 'istri', 'sela', 'apa', 'profesi', '', 'kau', 'juara', 'sehat', 'gojek', 'moga', 'lacar', 'rejekimu', 'amiiin', '']\n",
      "1\t0.49354161\tb'bangga suami yg istri sela apa profesi  kau juara sehat gojek moga lacar rejekimu amiiin '\n",
      "\n",
      "\n",
      "tweet: Lucu.... suaranya kak abbey cantik kayak orang nya.... ?????????????? terharu....\n",
      "proses tweet: ['lucu', '', 'suara', 'kak', 'abbey', 'cantik', 'kayak', 'orang', 'nya', '', 'haru', '']\n",
      "1\t0.49134641\tb'lucu  suara kak abbey cantik kayak orang nya  haru '\n",
      "\n",
      "\n",
      "tweet: Pernah ketemu sama mba artika sari disesemall di Jakarta...ampunnn ayunee...mana semampai....murah senyummm....cakep pisan lah\n",
      "proses tweet: ['pernah', 'ketemu', 'mba', 'artika', 'sari', 'disesemall', 'jakarta', '', 'ampunnn', 'ayunee', '', 'semampai', '', 'murah', 'senyummm', '', 'cakep', 'pis']\n",
      "1\t0.49057364\tb'pernah ketemu mba artika sari disesemall jakarta  ampunnn ayunee  semampai  murah senyummm  cakep pis'\n",
      "\n",
      "\n",
      "tweet: Semoga betul klo sma yg ini mh....klo bella mh udah dapet yg amat sangat pas lah????\n",
      "proses tweet: ['moga', 'klo', 'sma', 'yg', 'mh', '', 'klo', 'bella', 'mh', 'udah', 'dapet', 'yg', 'pas']\n",
      "1\t0.49260335\tb'moga klo sma yg mh  klo bella mh udah dapet yg pas'\n",
      "\n",
      "\n",
      "tweet: Duh geulis kitu ditinggal sama yg 'itu' duuuhhhh hahaha jodohnya pasti Allah kasih yg lebih baik Teeeh ??\n",
      "proses tweet: ['duh', 'geulis', 'kitu', 'tinggal', 'yg', 'itu', 'duuuhhhh', 'hahaha', 'jodoh', 'allah', 'kasih', 'yg', 'teeeh']\n",
      "1\t0.49473729\tb'duh geulis kitu tinggal yg itu duuuhhhh hahaha jodoh allah kasih yg teeeh'\n",
      "\n",
      "\n",
      "tweet: Di ulang berkali kali sampet gak percaya klo ini suara aslinya. Bagus bgt ??\n",
      "proses tweet: ['di', 'ulang', 'kali', 'kali', 'sampet', 'gak', 'percaya', 'klo', 'suara', 'asli', 'bagus', 'bgt']\n",
      "1\t0.49767222\tb'di ulang kali kali sampet gak percaya klo suara asli bagus bgt'\n",
      "\n",
      "\n",
      "tweet: Laki-laki yang beneran berjuang unt anak istri, membahagiakan istri insyaallah di dunia mendapat rejeki yg ga disangka-sangka di akherat akan ada surga kelak amiin\n",
      "proses tweet: ['laki', 'beneran', 'juang', 'unt', 'anak', 'istri', 'bahagia', 'istri', 'insyaallah', 'dunia', 'rejeki', 'yg', 'ga', 'sangka', 'akherat', 'surga', 'kelak', 'amiin']\n",
      "1\t0.49448478\tb'laki beneran juang unt anak istri bahagia istri insyaallah dunia rejeki yg ga sangka akherat surga kelak amiin'\n",
      "\n",
      "\n",
      "tweet: Doa istri dan anak menyertaimu bang... lelaki yg bertanggung jawab itu sangat PERLU, karna kalo laki2 tanggung jawab otomatis dia akan sayang dgn anak dan istri. Beda dgn sayang yg belum tentu bertanggung jawab ??????\n",
      "proses tweet: ['doa', 'istri', 'anak', 'serta', 'bang', '', 'lelaki', 'yg', 'tanggung', 'perlu', 'karna', 'kalo', 'laki2', 'tanggung', 'otomatis', 'sayang', 'dgn', 'anak', 'istri', 'beda', 'dgn', 'sayang', 'yg', 'tanggung']\n",
      "1\t0.48887792\tb'doa istri anak serta bang  lelaki yg tanggung perlu karna kalo laki2 tanggung otomatis sayang dgn anak istri beda dgn sayang yg tanggung'\n",
      "\n",
      "\n",
      "tweet: Semangat mebangun indonesia yg lebih maju pak jokowi ????\n",
      "proses tweet: ['semangat', 'mebangun', 'indonesia', 'yg', 'maju', 'jokowi']\n",
      "1\t0.49736880\tb'semangat mebangun indonesia yg maju jokowi'\n",
      "\n",
      "\n",
      "tweet: Ibu tiri terhebat..... Ntr palingan klo dah 20an dia baru perawatan .skrg msh muda kasian klo di paksa perawatan .cm ya paling tutup mata tutup kuping skrg mah\n",
      "proses tweet: ['ibu', 'tiri', 'hebat', '', '', 'ntr', 'paling', 'klo', 'dah', '20an', 'awat', 'skrg', 'msh', 'muda', 'kasi', 'klo', 'paksa', 'awat', 'cm', 'tutup', 'mata', 'tutup', 'kuping', 'skrg', 'mah']\n",
      "1\t0.49347983\tb'ibu tiri hebat   ntr paling klo dah 20an awat skrg msh muda kasi klo paksa awat cm tutup mata tutup kuping skrg mah'\n",
      "\n",
      "\n",
      "tweet: Ya ampun lakikny ganteng. Rini nya juga cantik. Mungil pula skrg.\n",
      "proses tweet: ['ya', 'ampun', 'lakikny', 'ganteng', 'rini', 'nya', 'cantik', 'mungil', 'skrg']\n",
      "1\t0.49942685\tb'ya ampun lakikny ganteng rini nya cantik mungil skrg'\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Some error analysis \n",
    "print('Label Predicted Tweet')\n",
    "for x,y in zip(X_test,y_test):\n",
    "    y_hat = predict_tweet(x, freqs, theta)\n",
    "    if np.abs(y - (y_hat > 0.5)) > 0:\n",
    "        print('tweet:', x)\n",
    "        print('proses tweet:', process_tweet(x))\n",
    "        print('%d\\t%0.8f\\t%s' % (y, y_hat, ' '.join(process_tweet(x)).encode('ascii', 'ignore')))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict own tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loo', 'cari', 'gara-gara']\n",
      "[[0.4997834]]\n",
      "Negative sentiment\n"
     ]
    }
   ],
   "source": [
    "# coba predik\n",
    "my_tweet = \"\"\"\n",
    "kenapa loo, mau cari gara-gara\n",
    "\"\"\"\n",
    "print(process_tweet(my_tweet))\n",
    "y_hat = predict_tweet(my_tweet, freqs, theta)\n",
    "print(y_hat)\n",
    "if y_hat > 0.5:\n",
    "    print('Positive sentiment')\n",
    "else: \n",
    "    print('Negative sentiment')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "GQLN-l93iF2B",
    "LZMhPthAicoE",
    "Nh5funiyhB5G",
    "hEOOD0DD0HN9",
    "3nODmHg3LWsh",
    "TLe8VnOkMB7o",
    "GvIYq4rbRti6",
    "MQQiPZRWQ0cL",
    "YnrdXH8VTg54"
   ],
   "name": "instagram-sentimen-analisi.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:jcop_usl]",
   "language": "python",
   "name": "conda-env-jcop_usl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
